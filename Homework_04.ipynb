{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shelbymenown/Artificial-Intelligence-Coursework/blob/main/Homework_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LwvvMtG18SK"
      },
      "source": [
        "# Recurrent Neural Network Homework\n",
        "\n",
        "This is the 4th assignment for CAP 4630 and we will implement a basic RNN network and an LSTM network with Keras to solve two problems. \\\n",
        "You will use **\"Tasks\"** and **\"Hints\"** to finish the work. **(Total points 85, plus 15 bonus points)** \\\n",
        "You may use Machine Learning libaries like Scikit-learn for data preprocessing.\n",
        "\n",
        "**Task Overview:**\n",
        "- Implement a basic RNN network to solve time series prediction \n",
        "- Implement an LSTM network to conduct sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24oSrIK18SL"
      },
      "source": [
        "## 1 - Implement Basic RNN network with Keras to predict time series##\n",
        "### 1.1 Prepare the data (17 Points)\n",
        "\n",
        "Prepare time series data for deep neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the given train and test data: \"train.txt\" and \"test.txt\". **(5 Points)**\n",
        "2. Generate the **TRAIN** and **TEST** labels. **(5 Points)**\n",
        "2. Normalize the **TRAIN** and **TEST** data with sklearn function \"MinMaxScaler\". **(5 Points)**\n",
        "3. Print out the **TEST** data and label. **(2 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. The length of original train data is 113 which starts from **\"1949-01\"** to **\"1958-05\"**. The length of original test data is 29, which starts from **\"1958-07\"** to **\"1960-11\"**. \n",
        "2. Set the data types of both train and test data to \"float32\". \n",
        "3. When you prepared input data X (sequences) and oupt data Y (labels), please consider the following relationship:\n",
        "    - The sequence X should be the **past 12** datapoints in the time series, i.e., observation sequence with historical window of 12. You may check the time series data and think about the reason.\n",
        "    - The label Y should be the **next 1** datapoint in the time series (one point ahead prediction).\n",
        "4. The first 3 train data and label should be:\n",
        "\n",
        "trainX[0] = [[0.5801282 &nbsp; 0.625 &nbsp; 0.30128205 &nbsp;0.15705132 &nbsp;0. &nbsp; 0.08653843 &nbsp; 0.16025639 &nbsp; 0.1025641 &nbsp; 0.3076923 &nbsp; 0.27564108 &nbsp;0.3525641 &nbsp; 0.5192307]]\\\n",
        "trainY[0] = [0.7628205]\n",
        "\n",
        "trainX[1] = [[0.625   &nbsp;   0.30128205&nbsp; 0.15705132&nbsp; 0.   &nbsp;      0.08653843 &nbsp;0.16025639&nbsp; 0.1025641 &nbsp; 0. &nbsp;3076923&nbsp; 0.27564108 &nbsp;0.3525641&nbsp;  0.5192307&nbsp;  0.7628205 ]]\\\n",
        "trainY[1] = [0.798077]\n",
        "\n",
        "trainX[2] =  [[0.30128205 &nbsp; 0.15705132 &nbsp; 0.   &nbsp;   0.&nbsp; 08653843&nbsp;0.16025639&nbsp;0.1025641 &nbsp; 0.3076923 &nbsp;0.27564108 &nbsp;0.3525641 &nbsp; 0.5192307 &nbsp;0.7628205 &nbsp; 0.798077]]\\\n",
        "trainY[2] = [0.49038458]\n",
        "\n",
        "5. Apply the MinMaxScaler to both the train and test data.\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
        "\n",
        "\n",
        "6. After the preparation with scaler fitting, the shapes of trainX, trainY, testX, and testY are as follows:\\\n",
        "trainX.shape = (101, 1, 12)\\\n",
        "trainY.shape = (101,)\\\n",
        "testX.shape = (17, 1, 12)\\\n",
        "testY.shape = (17,)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS7xhrC3-_-1"
      },
      "source": [
        "### Import Libraries ###\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tensorflow.keras.layers \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "  tf.random.set_seed(seed_value)\n",
        "  np.random.seed(seed_value)\n",
        "  random.seed(seed_value)\n",
        "reset_random_seeds()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shAj2Y6IuxUv"
      },
      "source": [
        "### Prepare and Preprocess Data Here ###\n",
        "from pandas import read_csv\n",
        "\n",
        "### Design a Function to Prepare Observation Sequences and Corresponding Labels ###\n",
        "def create_dataset(dataset, look_back=12): # look_back is used to specify input sequence length\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back):\n",
        "        dataX.append(dataset[i:(i + look_back)]) # make sure correct start and end elements\n",
        "        dataY.append(dataset[i + look_back]) # make sure correct start and end elements; here, we have only one point ahead for prediction\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "### Train and Test Data Loading with float32 type ####\n",
        "dataframe_test = read_csv('test.txt', usecols=['Passengers'])\n",
        "dataset_test = dataframe_test.values\n",
        "dataset_test = dataset_test.astype('float32')\n",
        "\n",
        "dataframe_train = read_csv('train.txt', usecols=['Passengers'])\n",
        "dataset_train = dataframe_train.values\n",
        "dataset_train = dataset_train.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8MlgYTvvIeD"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "### Reshape Data ###\n",
        "np.reshape(dataset_test, (dataset_test.shape[0], 1))\n",
        "np.reshape(dataset_train, (dataset_train.shape[0], 1))\n",
        "\n",
        "### Scale Training and Test Data to [0, 1] ###\n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) # specify the scaler\n",
        "test = scaler.fit_transform(dataset_test) # fit the scaler to the test data\n",
        "train = scaler.fit_transform(dataset_train) # fit the scaler to the training data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA_n85TRQXxZ"
      },
      "source": [
        "### Train and Test Data Split\n",
        "trainX, trainY = create_dataset(train, look_back=12) # historical window is 12; future window is 1.\n",
        "testX, testY = create_dataset(test, look_back=12)\n",
        "\n",
        "### Train and Test Data Reshape (to fit RNN input)\n",
        "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1])) \n",
        "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ1uHWzX8wlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258b5d2c-1bec-484c-faf6-32e4a17f8fc2"
      },
      "source": [
        "# print out the test data and label here\n",
        "print(testX)\n",
        "print(testY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0.5801282  0.625      0.30128205 0.15705132 0.         0.08653843\n",
            "   0.16025639 0.1025641  0.3076923  0.27564108 0.3525641  0.5192307 ]]\n",
            "\n",
            " [[0.625      0.30128205 0.15705132 0.         0.08653843 0.16025639\n",
            "   0.1025641  0.3076923  0.27564108 0.3525641  0.5192307  0.7628205 ]]\n",
            "\n",
            " [[0.30128205 0.15705132 0.         0.08653843 0.16025639 0.1025641\n",
            "   0.3076923  0.27564108 0.3525641  0.5192307  0.7628205  0.798077  ]]\n",
            "\n",
            " [[0.15705132 0.         0.08653843 0.16025639 0.1025641  0.3076923\n",
            "   0.27564108 0.3525641  0.5192307  0.7628205  0.798077   0.49038458]]\n",
            "\n",
            " [[0.         0.08653843 0.16025639 0.1025641  0.3076923  0.27564108\n",
            "   0.3525641  0.5192307  0.7628205  0.798077   0.49038458 0.31089747]]\n",
            "\n",
            " [[0.08653843 0.16025639 0.1025641  0.3076923  0.27564108 0.3525641\n",
            "   0.5192307  0.7628205  0.798077   0.49038458 0.31089747 0.16666663]]\n",
            "\n",
            " [[0.16025639 0.1025641  0.3076923  0.27564108 0.3525641  0.5192307\n",
            "   0.7628205  0.798077   0.49038458 0.31089747 0.16666663 0.30448723]]\n",
            "\n",
            " [[0.1025641  0.3076923  0.27564108 0.3525641  0.5192307  0.7628205\n",
            "   0.798077   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868]]\n",
            "\n",
            " [[0.3076923  0.27564108 0.3525641  0.5192307  0.7628205  0.798077\n",
            "   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868 0.25961542]]\n",
            "\n",
            " [[0.27564108 0.3525641  0.5192307  0.7628205  0.798077   0.49038458\n",
            "   0.31089747 0.16666663 0.30448723 0.34294868 0.25961542 0.34935904]]\n",
            "\n",
            " [[0.3525641  0.5192307  0.7628205  0.798077   0.49038458 0.31089747\n",
            "   0.16666663 0.30448723 0.34294868 0.25961542 0.34935904 0.48397434]]\n",
            "\n",
            " [[0.5192307  0.7628205  0.798077   0.49038458 0.31089747 0.16666663\n",
            "   0.30448723 0.34294868 0.25961542 0.34935904 0.48397434 0.5192307 ]]\n",
            "\n",
            " [[0.7628205  0.798077   0.49038458 0.31089747 0.16666663 0.30448723\n",
            "   0.34294868 0.25961542 0.34935904 0.48397434 0.5192307  0.72115386]]\n",
            "\n",
            " [[0.798077   0.49038458 0.31089747 0.16666663 0.30448723 0.34294868\n",
            "   0.25961542 0.34935904 0.48397434 0.5192307  0.72115386 1.        ]]\n",
            "\n",
            " [[0.49038458 0.31089747 0.16666663 0.30448723 0.34294868 0.25961542\n",
            "   0.34935904 0.48397434 0.5192307  0.72115386 1.         0.94871795]]\n",
            "\n",
            " [[0.31089747 0.16666663 0.30448723 0.34294868 0.25961542 0.34935904\n",
            "   0.48397434 0.5192307  0.72115386 1.         0.94871795 0.6346154 ]]\n",
            "\n",
            " [[0.16666663 0.30448723 0.34294868 0.25961542 0.34935904 0.48397434\n",
            "   0.5192307  0.72115386 1.         0.94871795 0.6346154  0.48397434]]]\n",
            "[[0.7628205 ]\n",
            " [0.798077  ]\n",
            " [0.49038458]\n",
            " [0.31089747]\n",
            " [0.16666663]\n",
            " [0.30448723]\n",
            " [0.34294868]\n",
            " [0.25961542]\n",
            " [0.34935904]\n",
            " [0.48397434]\n",
            " [0.5192307 ]\n",
            " [0.72115386]\n",
            " [1.        ]\n",
            " [0.94871795]\n",
            " [0.6346154 ]\n",
            " [0.48397434]\n",
            " [0.25641024]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AlakqA_vuFb"
      },
      "source": [
        "### 1.2 - Build the RNN model (20 Points) ##\n",
        "\n",
        "\n",
        "Build an RNN model with SimpleRNN cell. \n",
        "\n",
        "**Tasks:**\n",
        "1. Build an RNN model with 1 RNN layer and 1 Dense layer.  **(10 Points)**\n",
        "2. Compile the model. **(5 Points)**\n",
        "3. Train the model for 100 epochs with **batch_size = 10**. **(5 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may consider **tensorflow.keras.layers.SimpleRNN(unit_size=4)** to specify RNN cells.\n",
        "2. Use loss function = 'mean_squared_error' and select **Adam** optimizer with **learning_rate=0.01** and other default settings.\n",
        "3. After first epoch, the train loss is changed to around **0.0656**. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn92qh8oyq0B"
      },
      "source": [
        "### Build the RNN Model ###\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "model = Sequential() # Declare Sequential class and assign it to variable \"model\"\n",
        "model.add(keras.layers.SimpleRNN(units=4)) # Add a simple RNN layer with unit_size=4 in the model \n",
        "model.add(keras.layers.Dense(units=1)) # Add a following Dense layer with units=1 in the model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnO-5WT-3hgH"
      },
      "source": [
        "### Compile the RNN model ###\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=opt) # model compiled with mean_squared_error loss and adam optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tpZAutlzify",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0b837e-e0ec-4414-deea-8ddc688b4bdf"
      },
      "source": [
        "### Train the RNN model and PRINT OUT MODEL STRUCTURE with model.summary() ###\n",
        "model.fit(trainX, trainY, batch_size=10, epochs=100,verbose=2) # model fit with epoch=100, batch_size=10; verbose=2 is optional.\n",
        "model.summary() # print out model structure with model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "11/11 - 0s - loss: 0.0656\n",
            "Epoch 2/100\n",
            "11/11 - 0s - loss: 0.0340\n",
            "Epoch 3/100\n",
            "11/11 - 0s - loss: 0.0215\n",
            "Epoch 4/100\n",
            "11/11 - 0s - loss: 0.0144\n",
            "Epoch 5/100\n",
            "11/11 - 0s - loss: 0.0125\n",
            "Epoch 6/100\n",
            "11/11 - 0s - loss: 0.0115\n",
            "Epoch 7/100\n",
            "11/11 - 0s - loss: 0.0092\n",
            "Epoch 8/100\n",
            "11/11 - 0s - loss: 0.0080\n",
            "Epoch 9/100\n",
            "11/11 - 0s - loss: 0.0068\n",
            "Epoch 10/100\n",
            "11/11 - 0s - loss: 0.0056\n",
            "Epoch 11/100\n",
            "11/11 - 0s - loss: 0.0047\n",
            "Epoch 12/100\n",
            "11/11 - 0s - loss: 0.0055\n",
            "Epoch 13/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Epoch 14/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Epoch 15/100\n",
            "11/11 - 0s - loss: 0.0034\n",
            "Epoch 16/100\n",
            "11/11 - 0s - loss: 0.0037\n",
            "Epoch 17/100\n",
            "11/11 - 0s - loss: 0.0028\n",
            "Epoch 18/100\n",
            "11/11 - 0s - loss: 0.0029\n",
            "Epoch 19/100\n",
            "11/11 - 0s - loss: 0.0029\n",
            "Epoch 20/100\n",
            "11/11 - 0s - loss: 0.0029\n",
            "Epoch 21/100\n",
            "11/11 - 0s - loss: 0.0022\n",
            "Epoch 22/100\n",
            "11/11 - 0s - loss: 0.0022\n",
            "Epoch 23/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 24/100\n",
            "11/11 - 0s - loss: 0.0024\n",
            "Epoch 25/100\n",
            "11/11 - 0s - loss: 0.0027\n",
            "Epoch 26/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 27/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 28/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 29/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 30/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 31/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 32/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 33/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 34/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 35/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 36/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 37/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 38/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 39/100\n",
            "11/11 - 0s - loss: 0.0019\n",
            "Epoch 40/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 41/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 42/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 43/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 44/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 45/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 46/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 47/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 48/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 49/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 50/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 51/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 52/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 53/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 54/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 55/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 56/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 57/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 58/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 59/100\n",
            "11/11 - 0s - loss: 0.0022\n",
            "Epoch 60/100\n",
            "11/11 - 0s - loss: 0.0035\n",
            "Epoch 61/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 62/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 63/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 64/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 65/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 66/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 67/100\n",
            "11/11 - 0s - loss: 0.0018\n",
            "Epoch 68/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 69/100\n",
            "11/11 - 0s - loss: 0.0023\n",
            "Epoch 70/100\n",
            "11/11 - 0s - loss: 0.0024\n",
            "Epoch 71/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 72/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 73/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 74/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 75/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 76/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 77/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 78/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 79/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 80/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 81/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 82/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 83/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 84/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 85/100\n",
            "11/11 - 0s - loss: 0.0020\n",
            "Epoch 86/100\n",
            "11/11 - 0s - loss: 0.0014\n",
            "Epoch 87/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 88/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 89/100\n",
            "11/11 - 0s - loss: 0.0011\n",
            "Epoch 90/100\n",
            "11/11 - 0s - loss: 0.0011\n",
            "Epoch 91/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 92/100\n",
            "11/11 - 0s - loss: 0.0017\n",
            "Epoch 93/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 94/100\n",
            "11/11 - 0s - loss: 0.0013\n",
            "Epoch 95/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 96/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 97/100\n",
            "11/11 - 0s - loss: 0.0016\n",
            "Epoch 98/100\n",
            "11/11 - 0s - loss: 0.0012\n",
            "Epoch 99/100\n",
            "11/11 - 0s - loss: 0.0015\n",
            "Epoch 100/100\n",
            "11/11 - 0s - loss: 0.0021\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 4)                 68        \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 73\n",
            "Trainable params: 73\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd2jZl4n0H8m"
      },
      "source": [
        "### 1.3 Evaluate Predictive Model Performance (10 Points)\n",
        "\n",
        "Predict datapoints with the observed datapoints and trained model. \n",
        "\n",
        "**Tasks:**\n",
        "1. Do direct prediction on train and test datapoints with the obtained model in section 1.2. **(2 Points)**\n",
        "2. Scale the prediction results back to original representation with the scaler. **(3 Points)**\n",
        "3. Calculate root mean squared error (RMSE) and **print out** the error for **both TRAIN and TEST**. **(3 Points)**\n",
        "4. **Plot** the **TEST** label and prediction. **(2 Points)**\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Scale back the predictions with the build-in function \"scaler.inverse_transform\".\\\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.inverse_transform\n",
        "2. For validation: Train Score: **~13 RMSE** Test Score: **~19 RMSE**\n",
        "3. The plot for validation is shown below (observation test data are blue and prediction results are orange):\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNEkAMxnz8Mq"
      },
      "source": [
        "### Make Predictions ###\n",
        "trainPredict = model.predict(trainX)\n",
        "testPredict = model.predict(testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbnRqEv9z-he"
      },
      "source": [
        "### Scale Back Predictions ###\n",
        "trainPredict = scaler.inverse_transform(trainPredict) # scale train prediction back with scaler.inverse_transform()\n",
        "trainY = scaler.inverse_transform([trainY[:,0]]) # scale train labels back with scaler.inverse_transform()\n",
        "\n",
        "testPredict = scaler.inverse_transform(testPredict) # scale test prediction back with scaler.inverse_transform()\n",
        "testY = scaler.inverse_transform([testY[:,0]]) # scale test labels back with scaler.inverse_transform()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdBWzmE91G6_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfddbd82-ef8c-4e11-92ca-b0f5a6fe4676"
      },
      "source": [
        "### Calculate Root Mean Squared Error (RMSE) ###\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error # Import mean_squared_error from sklearn.metrics\n",
        "\n",
        "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0])) \n",
        "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0])) \n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "print('Test Score: %.2f RMSE' % (testScore))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Score: 15.16 RMSE\n",
            "Test Score: 22.12 RMSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txdu8q7l1aju",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "39d33de4-3e62-4272-e3b8-01957fd35fec"
      },
      "source": [
        "### Plot Baseline and Predictions ###\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(testY[0]) \n",
        "plt.plot(testPredict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff21ab967b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O49Ug-FhtCg8"
      },
      "source": [
        "## 2 - Build an LSTM model to conduct sentiment analysis ##\n",
        "\n",
        "### 2.1 Prepare the data (13 Points) ###\n",
        "\n",
        "Prepare IMDB data for reccurent neural network training.\n",
        "\n",
        "**Tasks:**\n",
        "1. Load the data from IMDB review dataset and **print out** the lengths of sequences. **(3 Points)**\n",
        "2. Preprocess review data to meet the network input requirement by specifying **number of words=1000**, setting **the analysis length of the review = 100**, and **padding the input sequences**. **(10 Points)**\n",
        "\n",
        "**Hints:**  \n",
        "1. You may load the IMDB data with keras.datasets.imdb.load_data(num_words=max_features). Here. max_features is set to **1000**.\n",
        "2. You may use keras.preprocessing.sequence.pad_sequences(x_train, maxlen) to pad the input sequences and set maxlen to **100**.\n",
        "\n",
        "**Note:**\\\n",
        "We train the built LSTM-based model with ALL training data; the **validation set** (aka **development set**) is set with the **testing set** for model evaluation. This split is common in the application with limited sampled observation data, like NLP problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI4ki461S2V3"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "### Set random seed to ensure deterministic results\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvV1Sv2a18SM"
      },
      "source": [
        "# Prepare the data here\n",
        "\n",
        "# max_features =  # Only consider the top 1k words\n",
        "# maxlen =  # Only consider the first 100 words of each movie review\n",
        "\n",
        "# (x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data( ) # load IMDB data with specified num_words = 1000; testing set is set to validation set.\n",
        "# print(len(x_train), \"Training sequences\")\n",
        "# print(len(x_val), \"Validation sequences\")\n",
        "# x_train = keras.preprocessing.sequence.pad_sequences( ) # Pad IMDB training data with specified maxlen=100\n",
        "# x_val = keras.preprocessing.sequence.pad_sequences( ) # Pad IMDB validation data with specified maxlen=100\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_JFQeWK18SR"
      },
      "source": [
        "### 2.2 - Design and train LSTM model (25 Points) ###\n",
        "\n",
        "Build an LSTM model.\n",
        "\n",
        "**Tasks:**\n",
        "1. Build the LSTM model with **1 embedding layer**, **1 LSTM layer**, and **1 Dense layer**. **Print out** model summary. The embedding vector is specified with the dimension of **8**. **(10 Points)**\n",
        "2. Compile the LSTM model with **Adam** optimizer, **binary_crossentropy** loss function, and **accuracy** metrics. **(5 Points)**  \n",
        "3. Train the LSTM model with **batch_size=64 for 10 epochs** and report **training and validation accuracies over epochs**. **(5 Points)**\n",
        "4. **Print out** best validation accuracy. **(5 Points)**\n",
        "\n",
        "\n",
        "\n",
        "**Hints:**  \n",
        "1. Set input dimension to **1000** and output dimension to **8** for embedding layer.\n",
        "2. Set **unit_size=8** for LSTM layer.\n",
        "3. Set activation function to **sigmoid** for Dense layer.\n",
        "4. For validation: the outputs for first epoch should be close tobut maybe not exactly following the statistics below:\\\n",
        "**loss: ~0.5675 - accuracy: ~0.7072 - val_loss: ~0.4549 - val_accuracy: ~0.8020**\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDqqgFt118SS"
      },
      "source": [
        "### Model design with Embedding and LSTM layers ####\n",
        "# inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "# x = layers.Embedding( )(inputs) # Embed data in an 8-dimensional vector\n",
        "# x = layers.LSTM( )(x) # Add 1st layer of LSTM with 8 hidden states (aka units)\n",
        "# outputs = layers.Dense( )(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "# model = keras.Model(inputs, outputs) # Build new keras model\n",
        "# model.summary() # Print out model summary\n",
        "\n",
        "# model.compile( ) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "# model.fit( ) # Train the compiled model with model.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vqvy2tdEw7J"
      },
      "source": [
        "### 2.3 - LSTM hyperparameter tuning (Bonus 15 Points) ###\n",
        "\n",
        "Boost the performance of obtained LSTM (aka vanilla model) by hyperparameter tuning.\n",
        "\n",
        "**Tasks:**\n",
        "- All modificiations are directly conducted based on the vanilla model above (from 2.2).\n",
        "- For each scenario, **report <span style=\"color:red\"> BEST Validation Accuracy </span> and generate Training/Validation <span style=\"color:red\"> Accuracy plots over epochs</span>**. You may just paste the plot figures in the cells with **Markdown mode**. Make sure it is correctly shown after jupyter notebook run the cell.\n",
        "1.  Scenario 1 (**5 points**):\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 16.\n",
        "    - Modify the units of LSTM to 16.\n",
        "2. Scenario 2 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "3. Scenario 3 (**5 points**)\n",
        "    - Add one additional LSTM layer (totally 2 LSTM layers).\n",
        "    - Modify the embedding dimension to 128.\n",
        "    - Modify the units of LSTM to 128.\n",
        "    - Increase analysis length for review data to maxlen = 200\n",
        "\n",
        "**Hints:**  \n",
        "For validation: the outputs for first epoch should be close tobut maybe not exactly following the statistics below:\n",
        "- Scenario 1: **loss: ~0.4968 - accuracy: ~0.7450 - val_loss: ~0.4079 - val_accuracy: ~0.8198**\n",
        "- Scenario 2: **loss: ~0.4764 - accuracy: ~0.7670 - val_loss: ~0.4133 - val_accuracy: ~0.8179**\n",
        "- Scenario 3: **loss: ~0.4819 - accuracy: ~0.7644 - val_loss: ~0.4031 - val_accuracy: ~0.8105**\n",
        "\n",
        "You may follow the example from the reference below to add additional LSTM layer.\n",
        "\n",
        "**Useful Reference:**\n",
        "1. https://keras.io/examples/nlp/bidirectional_lstm_imdb/  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keod5xXkEKnx"
      },
      "source": [
        "########################### Scenario 1 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "# max_features =  # Only consider the top 1k words\n",
        "# maxlen =  # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "# inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "# x = layers.Embedding( )(inputs) # Embed data in a 16-dimensional vector\n",
        "# x = layers.LSTM( )(x) # Add 1st layer of LSTM with 16 hidden states (aka units); set return_sequences=true.\n",
        "# x = layers.LSTM( )(x) # Add 2nd layer of LSTM with 16 hidden states (aka units)\n",
        "# outputs = layers.Dense( )(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "# model = keras.Model(inputs, outputs) # Build new keras model\n",
        "# model.summary() # Print out model summary\n",
        "\n",
        "# model.compile( ) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "# model.fit( ) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmObp_VuQXxa"
      },
      "source": [
        "########################### Scenario 2 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "# max_features =   # Only consider the top 1k words\n",
        "# maxlen =  # Only consider the first 100 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "# inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "# x = layers.Embedding( )(inputs) # Embed data in a 128-dimensional vector\n",
        "# x = layers.LSTM( )(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "# x = layers.LSTM( )(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "# outputs = layers.Dense( )(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "# model = keras.Model(inputs, outputs) # Build new keras model\n",
        "# model.summary() # Print out model summary\n",
        "\n",
        "# model.compile( ) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "# model.fit( ) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHWCyXkKQXxa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "c0491d81-373a-4f61-d584-af7c6ae91220"
      },
      "source": [
        "########################### Scenario 3 ###########################\n",
        "##################################################################\n",
        "\n",
        "### Set random seed to ensure deterministic results ###\n",
        "import os\n",
        "seed_value = 1\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "def reset_random_seeds():\n",
        "   tf.random.set_seed(seed_value)\n",
        "   np.random.seed(seed_value)\n",
        "   random.seed(seed_value)\n",
        "\n",
        "reset_random_seeds() # randomly set initial data\n",
        "\n",
        "max_features =   # Only consider the top 1k words\n",
        "maxlen = # Only consider the first 200 words of each movie review\n",
        "\n",
        "### Model design with Embedding and LSTM layers ####\n",
        "# inputs = keras.Input(shape=(None,), dtype=\"int32\") # This is an easy way to set an adaptive length for input sequence\n",
        "# x = layers.Embedding( )(inputs) # Embed data in a 128-dimensional vector\n",
        "# x = layers.LSTM( )(x) # Add 1st layer of LSTM with 128 hidden states (aka units); set return_sequences=true.\n",
        "# x = layers.LSTM( )(x) # Add 2nd layer of LSTM with 128 hidden states (aka units)\n",
        "# outputs = layers.Dense( )(x) # Add a classifier with units=1 and activation=\"sigmoid\"\n",
        "\n",
        "### Clear cached model to refresh memory and build new model for training ###\n",
        "keras.backend.clear_session() # Clear cached model\n",
        "# model = keras.Model(inputs, outputs) # Build new keras model\n",
        "# model.summary() # Print out model summary\n",
        "\n",
        "# model.compile( ) # Compile built model with \"adam\", \"binary_crossentropy\", and metrics=[\"accuracy\"]\n",
        "# model.fit( ) # Train the compiled model using model.fit() with batch_size=64, epochs=10, and validation_data=(x_val, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-94-41607007d8bc>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    max_features =   # Only consider the top 1k words\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}